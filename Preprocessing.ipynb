{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/ag_news_csv/' #location of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(data_path, \"test.csv\"), names = [\"class\", \"title\", \"text\"]) # read test_data\n",
    "train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"), names = [\"class\", \"title\", \"text\"]) # read train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    text = str(text).replace(\"\\\\\", \"\")\n",
    "    \n",
    "    stop = set(stopwords.words('english')) # get stopwords\n",
    "    porter_stemmer = PorterStemmer() # stemmer\n",
    "\n",
    "    tokens = [word.lower() for sentence in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sentence)] # tokenize\n",
    "    \n",
    "    tokens = [porter_stemmer.stem(word) for word in tokens if word not in stop] # get rid of stop words\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word) > 2] # get rid of one size character \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens): # bigram function\n",
    "    \n",
    "    bigrm = [] \n",
    "    \n",
    "    for i,_ in enumerate(tokens[:-1]):\n",
    "        bigrm.append(tokens[i] + ' ' + tokens[i+1]) #put words next to each other into one \n",
    "        \n",
    "    return bigrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wall', 'st.', 'bear', 'claw', 'back', 'black', 'reuter']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wall st.',\n",
       " 'st. bear',\n",
       " 'bear claw',\n",
       " 'claw back',\n",
       " 'back black',\n",
       " 'black reuter']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if text_preprocessing and bigram works\n",
    "print(text_preprocessing(train_data['title'][0]))\n",
    "bigram(text_preprocessing(train_data['title'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing starts\n",
      "preprocessing finished\n",
      "Preprocessing complete in 14m 9s\n"
     ]
    }
   ],
   "source": [
    "print(\"preprocessing starts\")\n",
    "\n",
    "start_time = time.time() #starting time\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = [], [], [], [] # list for each train_x, train_y, test_x, test_y\n",
    "\n",
    "# for train data\n",
    "for i, row in train_data.iterrows(): # put all the tokens resulted from text_preprocessing texts and titles and bigramming texts and titles\n",
    "    tokens = text_preprocessing([row['title']]) + bigram(text_preprocessing([row['title']])) + text_preprocessing(row['text']) + bigram(text_preprocessing(row['text']))\n",
    "    train_X.append(tokens)\n",
    "    \n",
    "    cls = row['class'] - 1\n",
    "    train_Y.append(cls)\n",
    "    \n",
    "# for test data\n",
    "for i, row in test_data.iterrows():\n",
    "    tokens = text_preprocessing([row['title']]) + bigram(text_preprocessing([row['title']])) + text_preprocessing(row['text']) + bigram(text_preprocessing(row['text']))\n",
    "    test_X.append(tokens)\n",
    "    \n",
    "    cls = row['class'] - 1\n",
    "    test_Y.append(cls)\n",
    "\n",
    "print(\"preprocessing finished\")\n",
    "time_elapsed = time.time() - start_time # get the time it took to preprocess\n",
    "print('Preprocessing complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = {'train_X':train_X,'train_Y':train_Y,'test_X':test_X,'test_Y':train_Y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"wb\") as fw:\n",
    "    pickle.dump(dataDict, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
